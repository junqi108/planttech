{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import os, sys, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "from scipy.stats import chisquare\n",
    "import pandas as pd\n",
    "import pyrr\n",
    "from tqdm import tqdm\n",
    "\n",
    "basedir = os.path.dirname(os.getcwd())\n",
    "_py = os.path.join(basedir, 'py')\n",
    "_data = os.path.join(basedir, 'data')\n",
    "_images = os.path.join(basedir, 'images')\n",
    "\n",
    "sys.path.insert(1, _py)\n",
    "import loads\n",
    "import lia\n",
    "import ray as rayt\n",
    "import lad\n",
    "import figures\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib qt\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 2021-10-06 11:03:10,766 - utils - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Structure\n",
    "\n",
    "The following include the process to extract the `Leaaf Area Density` (LAD) from a mock point cloud generated in `Blensor` software. This process is splitted in 3 main cores:\n",
    "\n",
    "* Preparation of data\n",
    "* Leaf Inclination Angle (LIA) estimation\n",
    "* Leaf Area Density (LAD) estimation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparation of data\n",
    "\n",
    "In order to get the LIA and hence the LAD, we need to segmentated the trees and the leaves.\n",
    "\n",
    "First, we define the name of the directory where the Blensor output data is, in this particular case we will look for directory `test`. Pipeline will look for this directory inside the `data` directory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "mockname = 'test'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we convert Blensor output txt files that have fake numpy extension to real npy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "loads.numpy2npy(mockname)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we create the module to segmentate trees. This will be tuned acordingly for each data set, so below module only works for this particular data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def segtree(df, show=False):\n",
    "\n",
    "    trees = {}\n",
    "\n",
    "    if show:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # centres\n",
    "    x, y = [0], [0]\n",
    "    num = 0\n",
    "    dx, dy = 5, 5\n",
    "\n",
    "    for i in x:\n",
    "        for j in y:\n",
    "            \n",
    "            keep = np.ones(len(df['x']), dtype=bool)\n",
    "            keep &= (df['x'] < i+dx) & (df['x'] > i-dx)\n",
    "            keep &= (df['y'] < j+dy) & (df['y'] > j-dy)\n",
    "\n",
    "            trees['tree_%s' %(str(num))] = keep\n",
    "            \n",
    "            if show:\n",
    "                plt.scatter(df['x'][leaves & keep], df['y'][leaves & keep], s=0.5, label=num)\n",
    "                        \n",
    "            num += 1\n",
    "\n",
    "    if show:\n",
    "        plt.legend()\n",
    "    \n",
    "    return trees\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it! Now we can segmentate trees. First, with function `npy2pandas` we load all the `npy` files into a pandas DataFrame (DF). Then, since this is a mockup dataset, we can easily separate the leaves from everythin else in the point cloud (PC). We do this with function `extract_leaves` that requires the pandas DF as input. Finally, we invoke the above module to segmentate trees that requires the pandas DF as well.\n",
    "\n",
    "outputs from this are:\n",
    "\n",
    "* `df`: Pandas DF with the entire PC\n",
    "* `leaves`: Boolean array of PC dimensions with True for Points concerning leaves only\n",
    "* `trees`: python dictionary where each entry contains one tree in the form of boolena array with PC dimensions\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# load data into a pandas data frame\n",
    "df = loads.npy2pandas(mockname)\n",
    "# extract leaves. Boolean array output\n",
    "leaves = loads.extract_leaves(df, show=False)\n",
    "# extract trees. Dictionary with boolean arrays output\n",
    "trees = segtree(df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "s0600000.npy (22500, 16) (22500, 3)\n",
      "s0700000.npy (22500, 16) (22500, 3)\n",
      "s0100000.npy (22500, 16) (22500, 3)\n",
      "s0500000.npy (22500, 16) (22500, 3)\n",
      "s0400000.npy (22500, 16) (22500, 3)\n",
      "s0200000.npy (22500, 16) (22500, 3)\n",
      "s0300000.npy (22500, 16) (22500, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below piece of code shows an example of how to visualize the leaves points from one tree only."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# show the point cloud from leaves of firs tree only\n",
    "keep = (trees['tree_0']) & (leaves)\n",
    "loads.showPCfromDF(df[keep])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1;33m[Open3D WARNING] GLFW Error: Cocoa: Failed to find service port for display\u001b[0;m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute the `Leaf Inclination Angle` (LIA) for each tree\n",
    "\n",
    "The function that computes the LIA `leaf_angle` uses a KDtree approximation and the steps are as follow: \n",
    "\n",
    "a) `Compute normals`: This method fits a plane based on the nearesth neighbors for each point and gets the normal of this plane.\n",
    "    \n",
    "b) `Compute zenith angles`: Then, using the dot product with get the angle with respect to the zenith (i.e. agains vector (0, 0, 1)) \n",
    "    \n",
    "c) `Range correction`: The results angles run from $0 < \\theta < 180$, however we require these to be in the range $0 < \\theta < 90$ therefore we transfom those angles $> 90$ with relation:\n",
    "\n",
    "$$\\theta_{L} = 180 - \\theta$$\n",
    "\n",
    "d) `Weights correction`: The resulting LIA is biased to PC density and completeness. In order to reduce this biases, we compute weights via voxelization,\n",
    "\n",
    "$$\\eta_{i} = n_{i}/L^{3} \\\\\n",
    "  \\bar{\\eta} = \\frac{1}{N}\\sum_{i=0}^{N} \\eta{i}$$\n",
    "\n",
    "where $n_{i}$ is the number of points within voxel $i$, $L$ is the voxel size, and $N$ is the total number of voxels, then $\\eta_{i}$ is the volume density of voxel $i$, and $\\bar{\\eta}$ is the mean volume density.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function `leaf_angle` has to be ran per tree and requieres 6 input parameters:\n",
    "\n",
    "* `points`: $x$, $y$ and $z$ coordinates of the leaf point cloud (LPC).\n",
    "* `mockname`: name of directory where the data is.\n",
    "* `treename`: name/index of tree.\n",
    "* `voxel_size_w`: voxel size for `weights correction` i.e. $L$.\n",
    "* `kd3_sr`: KDtree searching radius for the nearest neighboors serch.\n",
    "* `max_nn`: Maximum number of nearest neightbors to be considered.\n",
    "\n",
    "This function returns a set of files inside directory `lia`:\n",
    "\n",
    "* `angles_<treename>.npy`: LIA for the LPC. One file per tree.\n",
    "* `weights_<treename>.npy`: LIA weights for the LPC. One file per tree.\n",
    "* `leaf_angle_dist_<treename>.png`: Figure of LIA ($\\theta_{L}$) distribution with `weights correction`. If `Truth` LIA available, this will be shown alongside. One figure per tree.\n",
    "* `leaf_angle_dist_height_<treename>.png`: Top - Figure of LPC distribution accross different heights in terms of voxels $k$. Bottom - If `Truth` LIA available, $\\theta_{L}^{truth} - \\theta_{L}$. The different curves show this for different heights ($k$). One figure per tree."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If truth LIA available i.e. there's a mesh file `mesh.ply` in the root direcory, then we will be able to run function `bestfit_pars_la` which essentialy runs `leaf_angle` for a range of values in `voxel_size_w`, `kd3_sr` and `max_nn` and find the best-fit for these three based on the minimal $\\chi^{2}$ between the estimated LIA and the truth LIA.\n",
    "\n",
    "`bestfit_pars_la` is as well ran per tree and requires only `points`, `mockname` and `treename`. It returns `bestfit_<treename>.npy` file that contains the `voxel_size_w`, `kd3_sr` and `max_nn` best-fit values per tree. it also returns a dictionary with the $\\chi^{2}$ for each of these runs.\n",
    "\n",
    "Using output dictionary from `bestfit_pars_la` we can run `bestfit_pars_la` to create figure `bestfits_pars_treename>.png` that shows the $\\chi^{2}$ for all the ranges used in `voxel_size_w`, `kd3_sr` and `max_nn`.\n",
    "\n",
    "## TO-DO:\n",
    "\n",
    "Current LIA implementation can work without `Truth` LIA, however, we need the `Truth` to estimate the optimal `voxel_size_w`, `kd3_sr` and `max_nn` parameters. We need to find the relation between these three and LPC that could rely on the LPC density, leaf size, leaf area, etc.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The piece of code bellow runs `bestfit_pars_la` and `best_fit_pars_plot` for each tree."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for key, val in trees.items():\n",
    "\n",
    "    keep = (val) & (leaves)\n",
    "    df_ = df[['x', 'y', 'z']][keep]\n",
    "    points = loads.DF2array(df_)\n",
    "    res = lia.bestfit_pars_la(points, mockname, treename=key)\n",
    "    lia.best_fit_pars_plot(res, key, mockname)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "voxel_size_w 0.0001 DONE...\n",
      "voxel_size_w 0.001 DONE...\n",
      "voxel_size_w 0.01 DONE...\n",
      "voxel_size_w 0.1 DONE...\n",
      "voxel_size_w 1 DONE...\n",
      "voxel_size_w BESTFIT:\t 0.01\n",
      "kd3_sr 0.001 DONE...\n",
      "kd3_sr 0.01 DONE...\n",
      "kd3_sr 0.1 DONE...\n",
      "kd3_sr 1.0 DONE...\n",
      "kd3_sr BESTFIT:\t 1.0\n",
      "max_nn 3 DONE...\n",
      "max_nn 5 DONE...\n",
      "max_nn 10 DONE...\n",
      "max_nn 20 DONE...\n",
      "max_nn 50 DONE...\n",
      "max_nn 100 DONE...\n",
      "max_nn BESTFIT:\t 5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "once we find the best-fit parameters we use them and run `leaf_angle` and get the LIA and corresponding weigths per tree. This is shown bellow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# load bestfit results\n",
    "for key, val in trees.items():\n",
    "\n",
    "    keep = (val) & (leaves)\n",
    "    df_ = df[['x', 'y', 'z']][keep]\n",
    "    points = loads.DF2array(df_)\n",
    "\n",
    "    bestfit_file = os.path.join(_data, mockname, 'lia', 'bestfit_%s.npy' %(key))\n",
    "    res = np.load(bestfit_file, allow_pickle=True)\n",
    "    res = res.tolist()\n",
    "\n",
    "    text = 'leaf area=%.2f \\n %s=%.4f \\n %s=%.4f \\n %s=%.4f ' %(res['leafsize'], 'voxel_size_w', res['voxel_size_w_bestfit'],'kd3_sr', res['kd3_sr_bestfit'],'max_nn', res['max_nn_bestfit'])\n",
    "    print(text)\n",
    "\n",
    "    chi2 = lia.leaf_angle(points, mockname, key, res['voxel_size_w_bestfit'], \n",
    "                            res['kd3_sr_bestfit'], res['max_nn_bestfit'], save=True,\n",
    "                                savefig=True, text=text)\n",
    "                                "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "leaf area=0.04 \n",
      " voxel_size_w=0.0100 \n",
      " kd3_sr=1.0000 \n",
      " max_nn=5.0000 \n",
      "0.0009380330384670287\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# `\u001dLeaf Area Density` (LAD)\n",
    "\n",
    "The LAD method implemented here uses the `Voxel 3D contact-bases frecuency` method first introduced by `HOSOI AND OMASA: VOXEL-BASED 3-D MODELING OF INDIVIDUAL TREES FOR ESTIMATING LAD`.\n",
    "\n",
    "The model looks like:\n",
    "\n",
    "$$LAD(h, \\Delta H) = \\frac{1}{\\Delta H} \\sum_{k=m_{h}}^{m_{h}+\\Delta H} l(k),$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$l(k) = \\alpha(\\theta)N(k) \\\\\n",
    "    = \\alpha(\\theta) \\cdot \\frac{n_{I}(k)}{n_{I}(k) + n_{P}(k)}.$$\n",
    "\n",
    "$l(k)$ is the `Leaf Area Index` (LAI) of the kth horizontal layer of the voxel array within a plant region, $\\Delta H$ is the horizontal layer thickness, and $m_{h}$ and $m_{h}+\\Delta H$ are the voxel coordinates on the vertical axis equivalent to height $h$ and $h+\\Delta H$ in orthogonal coordinates ($h = \\Delta k \\times m_{h}$). The LAI of the kth horizontal layer $l(k)$ is the product of the contact frequency $N(k)$ of laser beams in the k$th layer and the coefficient $\\alpha(\\theta)$, which corrects for leaf inclination at laser incident zenith angle $\\theta$.\n",
    "\n",
    "$n_{I}(k)$ is the number of voxels where the laser beams is intercepted by the kth layer, $n_{P}(k)$ is the number of voxels where the laser beams passed through the kth layer, and $n_{I}(k) + n_{P}(k)$ is the total number of voxels where the incident laser beams reach the kth layer.\n",
    "\n",
    "Despite the complexity of this method, it requieres only one parameter, the `voxel_size`. We will introduce a second parameter, the `downsample` whose importance will be explained later. The main steps towards LAD estimation are:\n",
    "\n",
    "1) Computing $n_{P}(k)$\n",
    "2) Computing $n_{I}(k)$\n",
    "3) Computing $\\alpha(\\theta)$\n",
    "4) Estimate LAD\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this example, we will usea a `voxel_size` = 0.2 and `downsample` = 0.05 which means that we downsample our whole data to only $5\\%$. A reminder that as well as in for the LIA, the following process is per tree."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "downsample = 0.05\n",
    "voxel_size = 0.2\n",
    "# to check everything looks fine\n",
    "show = False\n",
    "sample = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing $n_{P}(k)$\n",
    "\n",
    "Seeing where the beam pass through in the voxelize `Plant Region` (PR) is a tipycal ray tracing problem and it's reduced to see whether the ray hit or not an axis align bounding box (AABB).\n",
    "\n",
    "The below module grabs the requiered downsample percentge of the data randomly and if it's the first time we ran this, it will create the directory `lad_<downsmple>`. All the subsequent results will be stored inside this directory. The first time a particular downsample is ran, it will store the file `inds.npy` containing a boolean array with size of the pandas DF. If we change the `voxel_size` but not the `downsample`, then the below module will look first for the `inds.npy` instead of searching for another random subsample, this to maintain uniformity between different voxels sizes approaches.\n",
    "\n",
    "The function `main` does the magic here, it has to be ran per tree and requires 6 input parameters:\n",
    "\n",
    "* `points`: $x$, $y$ and $z$ coordinates from the downsample data in the form of numpy array.\n",
    "* `sensors`: $x$, $y$ and $z$ coordinates of sensor responsible from each point in `points` parameter above.\n",
    "* `pointsPR`: `points` above filtered to the LPC.\n",
    "* `voxel_size`: Voxel Size.\n",
    "* `resdir`: Name of output directory for the specific `downsample`.\n",
    "* `treename`: Name/index of tree.\n",
    "\n",
    "NOTE: `pointsPR` is required to get the same voxelization dimensions as in $n_{I}$.\n",
    "\n",
    "This function returns two files:\n",
    "\n",
    "* `m3s_<treename>_<voxel_size>.npy`: numpy boolean 3D-array with number of voxels dimensions. True if a beam hit the voxel.\n",
    "* `m3count_<treename>_<voxel_size>.npy`: numpy 3D-array with number of voxels dimensions. Each entry contains the number of beams that passed trhough that voxel.\n",
    "\n",
    "### To-Do\n",
    "\n",
    "Note that this is the slowest module of the entire pipeline, taking up to 5 minutes  for sample of 10,000 beams. This can be improved easlily if binding with a C++ ray AABB module instead."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "\n",
    "POINTS = loads.DF2array(df[['x', 'y', 'z']])\n",
    "SENSORS = loads.DF2array(df[['sx', 'sy', 'sz']])\n",
    "\n",
    "if downsample is not None:\n",
    "\n",
    "    resdir = os.path.join(_data, mockname, 'lad_%s' %(str(downsample)))\n",
    "    if not os.path.exists(resdir):\n",
    "        os.makedirs(resdir)\n",
    "\n",
    "    outdir = os.path.join(resdir, 'inds.npy')\n",
    "    if os.path.exists(outdir):\n",
    "        print('inds file already exists for donwnsample of %.3f at %s' %(downsample, outdir))\n",
    "\n",
    "        inds = np.load(outdir)\n",
    "\n",
    "        points = POINTS[inds]\n",
    "        sensors = SENSORS[inds]\n",
    "\n",
    "    else:\n",
    "\n",
    "        print('inds not been created yet for donwnsample of %.3f' %(downsample))\n",
    "        idx = np.random.randint(0, len(df), int(len(df) * downsample))\n",
    "        inds = np.zeros(len(df), dtype=bool)\n",
    "        inds[idx] = True\n",
    "\n",
    "        points = POINTS[inds]\n",
    "        sensors = SENSORS[inds]\n",
    "\n",
    "        np.save(outdir, inds)\n",
    "\n",
    "else:\n",
    "\n",
    "    resdir = os.path.join(_data, mockname, 'lad')\n",
    "    if not os.path.exists(resdir):\n",
    "        os.makedirs(resdir)\n",
    "\n",
    "if sample is not None:\n",
    "\n",
    "    idx = np.random.randint(0, len(df), int(sample))\n",
    "    points = POINTS[idx]\n",
    "    sensors = SENSORS[idx]\n",
    "\n",
    "for key, val in trees.items():\n",
    "\n",
    "    inPR = (val) & (leaves) & (inds)\n",
    "    pointsPR = POINTS[inPR]\n",
    "    m3s, m3count= rayt.main(points, sensors, pointsPR, voxel_size, resdir, key, show=show)\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inds file already exists for donwnsample of 0.100 at /Users/omar/projects/planttech/data/test/lad_0.1/inds.npy\n",
      "max --> [62, 61, 39]\n",
      "min --> [0, 0, 0]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "14986it [03:43, 66.95it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tot vox: \t 156240\n",
      "voxels hitted: \t 135168\n",
      "Percentage of voxels hitted by beam: 0.87\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing $n_{I}(k)$\n",
    "\n",
    "The $n_{I}$ per voxel is computed in function `compute_attributes`. It essentialy voxelize the LPC to get the PR dimensions (which have to be the same as in $n_{P}$). Then, for a numpy boolean 3D-array with voxelize PR dimensions, we fill it with True if there's a point in the voxel.\n",
    "\n",
    "This function looks for previous `m3s_<treename>_<voxel_size>.npy` result and get the attributes in the form of the same size numpy 3D-array (`m3att`). The attributes are:\n",
    "\n",
    "* 1 if any LPC in that voxel\n",
    "* 2 if any beam pass trhough that  voxel\n",
    "* 3 if none of previous\n",
    "\n",
    "It requires 4 input parameters `pointsPR`, `resdir`, `voxel_size`, `treename` which were defined in section `Computing $n_{P}(k)$`. It returns the attributes numpy 3D-array."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing $\\alpha(\\theta)$\n",
    "\n",
    "$\\alpha(\\theta)$ is expressed in terms of $G(\\theta)$, \n",
    "\n",
    "$$\\alpha(\\theta) = \\frac{\\cos(\\theta)}{G(\\theta)},$$\n",
    "\n",
    "where $G(\\theta)$ is the mean projection of a unit leaf area on a plane perpendicular to the direction of the laser beam. This quantity is determined with the assumption that leaves are positioned symmetrically with respect to the azimuth anc can be represented as:\n",
    "\n",
    "$$G(\\theta) =  \\sum_{q=1}^{T_{q}} g(q) S(\\theta, \\theta_{L}(q))$$\n",
    "\n",
    "where $S(\\theta, \\theta_{L}(q))$ is expresed in terms of the leaf in-inclination angle $\\theta_{L}$ (the zenith angle of the normal to the leaf surface), and $\\theta$ is the laser-beam incident zenith angle:\n",
    "\n",
    "$$S(\\theta, \\theta_{L}) = \\cos\\theta \\cos \\theta_{L}, \\hspace{.5cm} \\textrm{for } \\theta \\leq \\pi/2 - \\theta_{L}$$\n",
    "$$S(\\theta, \\theta_{L}) = \\cos\\theta \\cos \\theta_{L} \\left[ 1 + \\frac{2}{\\pi}(\\tan x - x) \\right], \\hspace{.5cm} \\textrm{for } \\theta \\gt \\pi/2 - \\theta_{L}$$\n",
    "\n",
    "$$x = \\cos^{-1}\\left( \\cot \\theta \\cot \\theta_{L} \\right).$$\n",
    "\n",
    "Here $q$ is the leaf-inclination-angle class and Tq is the total number of leaf-inclination-angle classes. Thus, if there are $18$ leaf-inclination-angle classes from $0◦$ to $90◦$ ($Tq = 18$), then each class consists of a $5◦$ interval. For example, $q = 1$, $q = 9$, and $q = 16$ include the angles from $0◦$ to $4◦$, $40◦$ to $44◦$, and $75◦$ to $79◦$, respectively. $g(q)$ is the distribution of the leaf-inclination-angle class $q$, which is a ratio of the leaf area belonging to class $q$ to total leaf area; $θ_{L}(q)$ is the midpoint angle of class $q$, which is the leaf-inclination angle used to represent class $q$.\n",
    "\n",
    "This process is done trhough function `Gtheta`. In function `alpha_k` we compute $\\alpha(\\theta)$ for the median of $\\theta$ the Beam Inclination Angles (BIA) with respect to zenith in the Kth layer. We made use of the files `angles_<treename>.npy` and `weights_<treename>.npy` we store previously in the directory `lia` to get $g(q)$. The function `alpha_k` create three figures inside the `figures` directory:\n",
    "\n",
    "* `alphas_<treename>_<voxel_size>.png`: $\\alpha(\\theta)$ for $0 < \\theta < 90$. The black-dashed line is $\\alpha(\\theta)$ with $G(\\theta)$ using all the weighted LIAs, while the coloured-solid lines are with $G(\\theta)$ using the weighted LIAs only in the Kth layer. The yellow shadow shows the BIAs range, and the vertical red-dashed line shows the \"magic\" angle at $\\theta = 57.5$ where $\\alpha(\\theta)$ is close to a constant value of $1.1$.\n",
    "* `bia_<treename>_<voxel_size>.png`: The BIAs distribution pre and after angle range correction.\n",
    "* `bia_per_k_<treename>_<voxel_size>.png`: The BIAs distribution in the Kth layer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estimate LAD\n",
    "\n",
    "Now that we have $\\alpha(\\theta, K)$ in the Kth layer, we can compute the LAI and therefore, the LAD. We do this in function `get_LADS` which requires 4 input parameters:\n",
    "\n",
    "* `m3att`: The numpy 3D-array attributes.\n",
    "* `voxel_size`: Voxel Size.\n",
    "* `kbins`: $\\Delta H$ in lengths if K.\n",
    "* `alphas_k`: `alpha_k` function output.\n",
    "\n",
    "This returns a numpy 2D-array with the height and LAD for the corresponding height with zero being the bottom of the PR.\n",
    "\n",
    "Finally, with function `plot_lads` we plot LAD as a function of height for:\n",
    "\n",
    "1) Using correction of $\\alpha(\\theta, K)$ for the median of $\\theta$ in the Kth layer.\n",
    "2) Without $\\alpha(\\theta, K)$ correction\n",
    "3) Truth LAD from mesh file.\n",
    "\n",
    "This figure is saved in directory `figures` with name `LAD_<treename>_<voxel_size>.png`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "downsample = 0.10\n",
    "voxel_size = 0.15"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "if downsample is not None:\n",
    "    inds_file = os.path.join(resdir, 'inds.npy')\n",
    "    inds = np.load(inds_file)\n",
    "    resdir = os.path.join(_data, mockname, 'lad_%s' %(str(downsample)))\n",
    "    print('downsample:', downsample)\n",
    "else:\n",
    "    inds = np.ones(len(df), dtype=bool)\n",
    "    resdir = os.path.join(_data, mockname, 'lad')\n",
    "\n",
    "isfigures = os.path.join(resdir, 'figures')\n",
    "if not os.path.exists(isfigures):\n",
    "    os.makedirs(isfigures)\n",
    "\n",
    "print('voxel_size:', voxel_size)\n",
    "\n",
    "for key, val in trees.items():\n",
    "\n",
    "    inPR = (val) & (leaves) & (inds)\n",
    "    pointsPR = POINTS[inPR]\n",
    "    sensorsPR = SENSORS[inPR]\n",
    "\n",
    "    m3att = lad.compute_attributes(pointsPR, resdir, voxel_size, key)\n",
    "    # get in down sample boolean array for LPC size\n",
    "    inds_ = inds[(val) & (leaves)]\n",
    "    lias, ws = lad.downsample_lia(mockname, key, inds_)\n",
    "    voxk = lad.get_voxk(pointsPR, voxel_size)\n",
    "    bia = lad.get_bia(pointsPR, sensorsPR)\n",
    "    meshfile = lad.get_meshfile(mockname)\n",
    "\n",
    "    figext = '%s_%s' %(key, str(voxel_size))\n",
    "    # figext = None\n",
    "    alphas_k = lad.alpha_k(bia, voxk, lias, ws, resdir, meshfile, figext=figext, \n",
    "                            klia=False, use_true_lia=True)\n",
    "\n",
    "    kmax = m3att.shape[2]\n",
    "    kbins = int(kmax/15)\n",
    "    print(kbins)\n",
    "    \n",
    "    # lads_min = lad.get_LADS(m3att, voxel_size, kbins, alphas_k[:,2], 1)\n",
    "    # lads_max = lad.get_LADS(m3att, voxel_size, kbins, alphas_k[:,4], 1)\n",
    "    lads_mid = lad.get_LADS(m3att, voxel_size, kbins, alphas_k[:,6], 1)\n",
    "    lads_0 = lad.get_LADS(m3att, voxel_size, kbins, alphas_k[:,6]*0+1, 1.0)\n",
    "    lads_mesh = lad.get_LADS_mesh(meshfile, voxel_size, kbins, kmax)\n",
    "\n",
    "    lads = {'Truth':lads_mesh, 'Correction Mean':lads_mid, 'No Correction':lads_0}\n",
    "\n",
    "    savefig = os.path.join(resdir, 'figures','LAD_%s.png' %(figext))\n",
    "    figures.plot_lads(lads, savefig=savefig)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "downsample: 0.1\n",
      "voxel_size: 0.15\n",
      "max --> [62, 61, 39]\n",
      "min --> [0, 0, 0]\n",
      "foliage voxel dimensions: \t (63, 62, 40)\n",
      "ray tracker voxel dimensions: \t (63, 62, 40)\n",
      "Number of voxels ocupied by points cloud: \t 4494\n",
      "Number of voxels ocupied by beam points cloud: \t 135168\n",
      "Total number of voxels in plant regions: \t 156240\n",
      "Number of voxels with attribute 1: \t 4494\n",
      "Number of voxels with attribute 2: \t 130674\n",
      "Number of voxels with attribute 3: \t 21072\n",
      "2\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d949bede1d2d28195f90229403804ad1488824f6ca2cb71c0e4905baa8d9072d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('plant-env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}